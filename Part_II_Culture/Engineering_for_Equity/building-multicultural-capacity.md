# 建设多元文化能力
杰出工程师的标志之一是能够理解产品如何对不同的人类群体有利和不利。 工程师应该具备技术能力，但他们也应该有洞察力，知道什么时候建造什么时候不建造。 辨别力包括建立识别和拒绝导致不良结果的特征或产品的能力。 这是一个崇高而艰巨的目标，因为要成为一名优秀的工程师，需要大量的个人主义。 然而，要取得成功，我们必须将我们的关注点从我们自己的社区扩展到下一个十亿用户或可能被我们的产品剥夺权利或落后的当前用户。
<details> <summary>英文原文</summary><div style="border:1px solid #eee;padding:5px;background-color:#F2F2F2">
One mark of an exceptional engineer is the ability to understand how products can advantage and disadvantage different groups of human beings. Engineers are expected to have technical aptitude, but they should also have the discernment to know when to build something and when not to. Discernment includes building the capacity to identify and reject features or products that drive adverse outcomes. This is a lofty and difficult goal, because there is an enormous amount of individualism that goes into being a high-performing engineer. Yet to succeed, we must extend our focus beyond our own communities to the next billion users or to current users who might be disenfranchised or left behind by our products.
</div></details>
随着时间的推移，您可能会构建数十亿人每天使用的工具——影响人们如何看待人类生命价值的工具、监控人类活动的工具以及捕获和保存敏感数据的工具，例如他们孩子和爱人的图像 以及其他类型的敏感数据。 作为一名工程师，您可能拥有比您意识到的更多的力量：真正改变社会的力量。 在您成为杰出工程师的过程中，了解在不造成伤害的情况下行使权力所需的与生俱来的责任至关重要。 第一步是识别由许多社会和教育因素引起的偏见的默认状态。 认识到这一点后，您将能够考虑经常被遗忘的用例或可能因您构建的产品而受益或受到伤害的用户。
<details> <summary>英文原文</summary><div style="border:1px solid #eee;padding:5px;background-color:#F2F2F2">
Over time, you might build tools that billions of people use daily—tools that influence how people think about the value of human lives, tools that monitor human activity, and tools that capture and persist sensitive data, such as images of their children and loved ones, as well as other types of sensitive data. As an engineer, you might wield more power than you realize: the power to literally change society. It’s critical that on your journey to becoming an exceptional engineer, you understand the innate responsibility needed to exercise power without causing harm. The first step is to recognize the default state of your bias caused by many societal and educational factors. After you recognize this, you’ll be able to consider the often-forgotten use cases or users who can benefit or be harmed by the products you build.
</div></details>
该行业继续向前发展，以越来越快的速度为人工智能 (AI) 和机器学习构建新的用例。 为了保持竞争力，我们在建设高素质的工程和技术劳动力方面努力扩大规模和提高效率。 然而，我们需要停下来思考一个事实，即今天，有些人有能力设计技术的未来，而另一些人则没有。 我们需要了解我们构建的软件系统是否会消除整个人口体验共享繁荣和提供平等获得技术的潜力。
<details> <summary>英文原文</summary><div style="border:1px solid #eee;padding:5px;background-color:#F2F2F2">
The industry continues to move forward, building new use cases for artificial intelligence (AI) and machine learning at an ever-increasing speed. To stay competitive, we drive toward scale and efficacy in building a high-talent engineering and technology workforce. Yet we need to pause and consider the fact that today, some people have the ability to design the future of technology and others do not. We need to under‐ stand whether the software systems we build will eliminate the potential for entire populations to experience shared prosperity and provide equal access to technology.
</div></details>
从历史上看，公司在完成推动市场主导地位和收入的战略目标与可能减缓实现该目标的势头之间做出决定时，选择了速度和股东价值。 由于许多公司重视个人绩效和卓越，但往往未能有效地推动所有领域的产品资产问责制，这一趋势更加严重。 关注代表性不足的用户显然是促进公平的机会。 为了继续在技术领域保持竞争力，我们需要学习为全球公平而设计。
<details> <summary>英文原文</summary><div style="border:1px solid #eee;padding:5px;background-color:#F2F2F2">
Historically, companies faced with a decision between completing a strategic objective that drives market dominance and revenue and one that potentially slows momentum toward that goal have opted for speed and shareholder value. This tendency is exacerbated by the fact that many companies value individual performance and excellence, yet often fail to effectively drive accountability on product equity across all areas. Focusing on underrepresented users is a clear opportunity to pro‐ mote equity. To continue to be competitive in the technology sector, we need to learn to engineer for global equity.
</div></details>
今天，当公司设计技术来扫描、捕捉和识别走在街上的人时，我们会感到担忧。 我们担心隐私以及政府现在和将来如何使用这些信息。 然而，大多数技术人员没有必要的视角来了解代表性不足的群体，以了解种族差异对面部识别的影响，或者了解应用人工智能如何导致有害和不准确的结果。
<details> <summary>英文原文</summary><div style="border:1px solid #eee;padding:5px;background-color:#F2F2F2">
Today, we worry when companies design technology to scan, capture, and identify people walking down the street. We worry about privacy and how governments might use this information now and in the future. Yet most technologists do not have the requisite perspective of underrepresented groups to understand the impact of racial variance in facial recognition or to understand how applying AI can drive harmful and inaccurate results.
</div></details>
目前，人工智能驱动的面部识别软件继续对有色人种或少数民族不利。 我们的研究不够全面，也没有涵盖足够广泛的不同肤色。 如果训练数据和创建软件的人都只代表一小部分人，我们不能期望输出是有效的。 在这种情况下，我们应该愿意推迟开发，争取获得更完整、更准确的数据，以及更全面、更具包容性的产品。
<details> <summary>英文原文</summary><div style="border:1px solid #eee;padding:5px;background-color:#F2F2F2">
Currently, AI-driven facial-recognition software continues to disadvantage people of color or ethnic minorities. Our research is not comprehensive enough and does not include a wide enough range of different skin tones. We cannot expect the output to be valid if both the training data and those creating the software represent only a small subsection of people. In those cases, we should be willing to delay development in favor of trying to get more complete and accurate data, and a more comprehensive and inclusive product.
</div></details>
然而，数据科学本身对人类评估具有挑战性。 即使我们确实有代表性，训练集仍然可能存在偏差并产生无效结果。 2016 年完成的一项研究发现，执法部门的面部识别数据库中有超过 1.17 亿美国成年人。 5 由于对黑人社区的不成比例的监管和不同的逮捕结果，在使用此类数据库时可能存在种族偏见的错误率 面部识别。 尽管软件的开发和部署速度越来越快，但独立测试却并非如此。 为了纠正这种严重的失误，我们需要有诚信来放慢速度，并确保我们的输入包含尽可能少的偏差。 谷歌现在在人工智能的背景下提供统计培训，以帮助确保数据集没有本质上的偏见。
<details> <summary>英文原文</summary><div style="border:1px solid #eee;padding:5px;background-color:#F2F2F2">
Data science itself is challenging for humans to evaluate, however. Even when we do have representation, a training set can still be biased and produce invalid results. A study completed in 2016 found that more than 117 million American adults are in a law enforcement facial recognition database.5 Due to the disproportionate policing of Black communities and disparate outcomes in arrests, there could be racially biased error rates in utilizing such a database in facial recognition. Although the software is being developed and deployed at ever-increasing rates, the independent testing is not. To correct for this egregious misstep, we need to have the integrity to slow down and ensure that our inputs contain as little bias as possible. Google now offers statistical training within the context of AI to help ensure that datasets are not intrinsically biased.
</div></details>
因此，将您的行业经验的重点转移到包括更全面、多元文化、种族和性别研究的教育不仅是您的责任，也是您雇主的责任。 科技公司必须确保其员工不断获得专业发展，并且这种发展是全面的和多学科的。 要求不是一个人独自承担学习其他文化或其他人口统计的任务。 变革要求我们每个人，无论是个人还是作为团队的领导者，都投资于持续的专业发展，这不仅可以培养我们的软件开发和领导技能，还可以培养我们理解全人类不同经历的能力。
<details> <summary>英文原文</summary><div style="border:1px solid #eee;padding:5px;background-color:#F2F2F2">
Therefore, shifting the focus of your industry experience to include more comprehensive, multicultural, race and gender studies education is not only your responsibility, but also the responsibility of your employer. Technology companies must ensure that their employees are continually receiving professional development and that this development is comprehensive and multidisciplinary. The requirement is not that one individual take it upon themselves to learn about other cultures or other demo‐ graphics alone. Change requires that each of us, individually or as leaders of teams, invest in continuous professional development that builds not just our software development and leadership skills, but also our capacity to understand the diverse experiences throughout humanity.
</div></details>

